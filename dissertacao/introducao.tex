\chapter{Introduction}
\label{chap:Introduction}
One of the major IT companies nowadays, known as Google \cite{Google}, had the initial idea of a way to process a huge data volume generated by its servers. This approach would later be known as MapReduce, built by two separate steps, Map and Reduce, each step based on a functional language function. At the same time, a Yahoo! \cite{Yahoo} led project was starting the implementation of MapReduce for its own system, which would then become a whole new project, named Apache Hadoop \cite{Hadoop}.

Today the Apache Hadoop framework has a very active community of both developers and users, however there are some characteristics that weren't changed from the day the framework was first designed. Among these characteristics there is one very detrimental and prone to bad performance issues, which is the focus on homogeneous environments. It is known that maintaining a totally homogeneous environment is harder and harder as the time passes, requiring either a huge initial investment or a huge effort in order to replace faulty hardware without changing the component capability.

The MapReduce's task performance inside Hadoop is tightly tied to the scheduler \cite{CASH}. Since it is an open source project, it is possible to change the scheduler aiming to make it capable of better adapting to heterogeneity while at the same time presenting a performance improvement.

A key characteristic in Hadoop's transition to heterogeneous environments is the context-aware capability. The definition of context can vary from one application to another, but as a rule of thumb it is some information that the application can use as base for decision making. When an application is context-aware, it will detect and adapt to the changes in the environment \cite{Manuele}.

In the present work, the context to which the application will have to adapt is related to the physical configuration of the machines that compose the Hadoop cluster, allowing the scheduler to work with real data collected from the machines and not suppositions as the default Hadoop configuration implies. In a more complex degree, the present work is just a part of a bigger project called PER-MARE \cite{PER-MARE}, which has the objective of adapting to more environment variations, as the insertion and removal of nodes in real time.


%-------------------------------------------------------------------
\section{Objective}

The main objective of this work is to improve Hadoop through a context-aware scheduling, which will provide better performance and adaptation on heterogeneous environments.

%-------------------------------------------------------------------
\section{Motivation}

Today, some processing tasks that used to be made through huge mainframes and servers are gradually transitioning to big clusters, which are composed of computers with more accessible prices and easily bought in the market. 

Even though the Apache Hadoop framework has clusters as its target, it was designed and implemented under a specific assumption. The framework's better performance is achieved when it is running on a homogeneous cluster, in other words, when all nodes have the same resources. The problem is that given today's hardware development, it might take the system to a point where it is not possible or at least not profitable to maintain cluster homogeneity. 

Since the default configuration of the scheduler tells that every node on the cluster has the same amount of resources, if a more powerful node is inserted all that extra capacity will be wasted. This happens because the cluster will not collect the real configuration, but the parameter set in a XML file as the node capacity. The opposite is also troublesome, if a less powerful node is inserted, the cluster will use it as if it had more potential, possibly overloading that node with more tasks that it can handle and causing errors or performance issues.

The present work is relevant, as it's objective is based on adaptation and improvement of an already existent technology. With the improved scheduler, not only will the Apache Hadoop clusters have a possibility to improve cluster's resource utilization, as the framework itself will be better prepared and capable of adapting to new heterogeneous environments in an easier and smoother way.